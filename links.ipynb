{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eba61806-30b5-48c8-9171-603890abc2b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping page 1...\n",
      "Scraping page 2...\n",
      "Scraping page 3...\n",
      "Scraping page 4...\n",
      "Scraping page 5...\n",
      "Scraping completed and data saved to Link_ebay_scraped_data.csv\n",
      "182 item numbers have been extracted and saved to item_numbers.txt and item_numbers.json\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "import re  # Import regular expressions module\n",
    "import json\n",
    "\n",
    "# Set up Selenium and open the page\n",
    "options = webdriver.ChromeOptions()\n",
    "options.add_argument('--headless')  # Run browser in headless mode\n",
    "service = Service(r\"C:\\Users\\User\\Downloads\\chromedriver.exe\")  # Path to ChromeDriver\n",
    "driver = webdriver.Chrome(service=service, options=options)\n",
    "\n",
    "# Define the base URL\n",
    "base_url = 'https://www.ebay.com/b/Cell-Phones-Smartphones/9355/bn_320094'\n",
    "\n",
    "driver.get(base_url)\n",
    "\n",
    "# Allow the page to load\n",
    "time.sleep(3)\n",
    "\n",
    "# Initialize the lists for storing data\n",
    "data = []\n",
    "item_numbers = []  # List to store item numbers\n",
    "\n",
    "page_number = 1  # Start from the first page\n",
    "\n",
    "while True:\n",
    "    print(f\"Scraping page {page_number}...\")\n",
    "\n",
    "    # Get the page source and pass it to BeautifulSoup\n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "\n",
    "    # Scrape the first 200 items on the page\n",
    "    items = soup.find_all('li', class_='s-item', limit=200)\n",
    "\n",
    "    for item in items:\n",
    "        title = item.find('h3', class_='s-item__title')\n",
    "        price = item.find('span', class_='s-item__price')\n",
    "        link = item.find('a', class_='s-item__link')['href']\n",
    "\n",
    "        # Extract the item number using a regular expression\n",
    "        match = re.search(r'/itm/(\\d+)', link)\n",
    "        if match:\n",
    "            item_number = match.group(1)\n",
    "            clean_link = f'https://www.ebay.com/itm/{item_number}'  # Extract the dynamic item number\n",
    "            item_numbers.append(item_number)  # Store the item number\n",
    "\n",
    "        # Check if title and price exist to avoid NoneType errors\n",
    "        if title and price and match:\n",
    "            data.append({\n",
    "                'Title': title.text,\n",
    "                'Price': price.text,\n",
    "                'Link': clean_link\n",
    "            })\n",
    "\n",
    "    # Try to find the \"Next\" button on the page to go to the next page\n",
    "    next_button = soup.find('a', class_='pagination__next')\n",
    "\n",
    "    # If no next button is found, we've reached the last page\n",
    "    if not next_button:\n",
    "        break\n",
    "\n",
    "    # If the \"Next\" button is found, click it to go to the next page\n",
    "    next_page_link = next_button['href']\n",
    "    driver.get(next_page_link)\n",
    "\n",
    "    # Allow the next page to load\n",
    "    time.sleep(3)\n",
    "\n",
    "    page_number += 1  # Increment the page number\n",
    "\n",
    "# Close the browser\n",
    "driver.quit()\n",
    "\n",
    "# Save the data into a CSV file\n",
    "df = pd.DataFrame(data)\n",
    "df.to_csv('Link_ebay_scraped_data.csv', index=False)\n",
    "print(\"Scraping completed and data saved to Link_ebay_scraped_data.csv\")\n",
    "\n",
    "# Save the item numbers to a .txt file\n",
    "with open('item_numbers1.txt', 'w') as f:\n",
    "    f.write(','.join(item_numbers))\n",
    "\n",
    "# Save the item numbers to a .json file\n",
    "with open('item_numbers1.json', 'w') as f:\n",
    "    json.dump(item_numbers, f)\n",
    "\n",
    "print(f\"{len(item_numbers)} item numbers have been extracted and saved to item_numbers.txt and item_numbers.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5edf6978-aa53-47ef-b39a-37c9a9eefa0e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
